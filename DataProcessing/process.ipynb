{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USDA Fruit & Vegetables Data Processor\n",
    "\n",
    "This notebook implements the ETL pipeline for USDA Fruit and Vegetable retail price data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T11:11:32.074153Z",
     "iopub.status.busy": "2026-01-18T11:11:32.074015Z",
     "iopub.status.idle": "2026-01-18T11:11:32.281700Z",
     "shell.execute_reply": "2026-01-18T11:11:32.280725Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "from collections import defaultdict\n",
    "from decimal import Decimal, InvalidOperation\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import openpyxl\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "These constants define the processing behavior:\n",
    "- `LISTING_URL`: USDA data products page with download links\n",
    "- `OUTPUT_DIR`: Destination for generated CSV files (layout: `alternative/usda/fruitandvegetables/{series}.csv`). Defaults to `/temp-output-directory` (Datafleet cloud path). For local development, set `TEMP_OUTPUT_DIRECTORY=./output`.\n",
    "- `YEAR_REGEX`: Matches 4-digit years (1900-2099) in titles/filenames\n",
    "- Unit mappings: Canonical forms derived from empirical profiling of 365 XLSX files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T11:11:32.283670Z",
     "iopub.status.busy": "2026-01-18T11:11:32.283467Z",
     "iopub.status.idle": "2026-01-18T11:11:32.287379Z",
     "shell.execute_reply": "2026-01-18T11:11:32.286564Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Constants ---\n",
    "LISTING_URL = \"https://www.ers.usda.gov/data-products/fruit-and-vegetable-prices\"\n",
    "\n",
    "# Output directory: defaults to /temp-output-directory (\"Datafleet\" cloud path).\n",
    "# For local development, set TEMP_OUTPUT_DIRECTORY=./output to write to the repo's output folder.\n",
    "OUTPUT_DIR = Path(os.environ.get(\"TEMP_OUTPUT_DIRECTORY\", \"/temp-output-directory\")) / \"alternative/usda/fruitandvegetables\"\n",
    "\n",
    "YEAR_REGEX = re.compile(r\"\\b((?:19|20)\\d{2})\\b\")\n",
    "FOOTNOTE_REGEX = re.compile(r\"\\s*\\d+(?:,\\d+)*\\s*$\")\n",
    "FORM_CATEGORY_LABELS = {\n",
    "    \"fresh\",\n",
    "    \"canned\",\n",
    "    \"frozen\",\n",
    "    \"dried\",\n",
    "    \"juice\",\n",
    "    \"peas & carrots\",\n",
    "    \"green peas & carrots\",\n",
    "    \"succotash\",\n",
    "}\n",
    "\n",
    "# --- XLSX Structure Constants ---\n",
    "# Empirically profiled from 365 USDA XLSX files (2026-01-15):\n",
    "# - Header rows observed at index 0-1; we check up to MAX_HEADER_SEARCH_ROWS for safety margin\n",
    "# - Data rows have 7-9 columns; MIN_DATA_ROW_COLUMNS is the minimum for a valid data row\n",
    "MAX_HEADER_SEARCH_ROWS = 15\n",
    "MIN_DATA_ROW_COLUMNS = 7\n",
    "\n",
    "# Note: Extended descriptions like \"per pint (16 fluid ounces concentrate)\" match via substring.\n",
    "PRICE_UNIT_MAP = {\n",
    "    \"per pound\": \"per_pound\",\n",
    "    \"per pint\": \"per_pint\",\n",
    "}\n",
    "CUP_UNIT_MAP = {\n",
    "    \"pounds\": \"pounds\",\n",
    "    \"pound\": \"pounds\",\n",
    "    \"pints\": \"pints\",\n",
    "    \"fluid ounces\": \"fluid_ounces\",\n",
    "    \"fl oz\": \"fluid_ounces\",\n",
    "    \"fl. oz.\": \"fluid_ounces\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Utility functions for text processing and decimal formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T11:11:32.289031Z",
     "iopub.status.busy": "2026-01-18T11:11:32.288910Z",
     "iopub.status.idle": "2026-01-18T11:11:32.293217Z",
     "shell.execute_reply": "2026-01-18T11:11:32.292510Z"
    }
   },
   "outputs": [],
   "source": [
    "def slugify(text: str) -> str:\n",
    "    \"\"\"Convert text to lowercase slug with underscores.\"\"\"\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"_\", text.lower()).strip(\"_\")\n",
    "\n",
    "\n",
    "def collapse_whitespace(text: str) -> str:\n",
    "    \"\"\"Collapse multiple whitespace characters into single spaces.\"\"\"\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "\n",
    "def format_decimal_for_csv(value: Decimal | None) -> str:\n",
    "    \"\"\"Format decimal for CSV output, stripping trailing zeros.\"\"\"\n",
    "    return \"\" if value is None else format(value, \"f\").rstrip(\"0\").rstrip(\".\")\n",
    "\n",
    "\n",
    "def parse_decimal(value: object) -> Decimal | None:\n",
    "    \"\"\"Parse a cell value as Decimal. Handles int, float, and string inputs uniformly via str().\"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    text = str(value).strip()\n",
    "    if not text:\n",
    "        return None\n",
    "    try:\n",
    "        return Decimal(text)\n",
    "    except InvalidOperation:\n",
    "        return None\n",
    "\n",
    "\n",
    "def lookup_canonical_unit(raw_unit_text: object, unit_map: dict[str, str]) -> str:\n",
    "    \"\"\"Look up canonical unit form from raw XLSX text using substring matching.\n",
    "\n",
    "    Uses next() with generator expression - idiomatic Python for \"find first match or default\".\n",
    "    Equivalent to a for-loop with early return, but expresses intent more clearly.\n",
    "    \"\"\"\n",
    "    if raw_unit_text is None:\n",
    "        return \"\"\n",
    "    text = collapse_whitespace(str(raw_unit_text)).lower()\n",
    "    return next((canonical for pattern, canonical in unit_map.items() if pattern in text), \"\")\n",
    "\n",
    "\n",
    "def make_series_code(product: str, form: str) -> str:\n",
    "    \"\"\"Generate series code from product name and form.\"\"\"\n",
    "    return f\"{slugify(product)}_{slugify(form)}\"\n",
    "\n",
    "\n",
    "def normalize_cup_equivalent(size: Decimal | None, unit: str) -> tuple[Decimal | None, str]:\n",
    "    \"\"\"Normalize cup equivalent units: convert fluid_ounces to pints (16 fl oz = 1 pint).\"\"\"\n",
    "    if size is None or unit != \"fluid_ounces\":\n",
    "        return size, unit\n",
    "    return size / Decimal(\"16\"), \"pints\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download & Extract\n",
    "\n",
    "Downloads XLSX/ZIP files from the USDA website and extracts them to a temporary directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T11:11:32.294687Z",
     "iopub.status.busy": "2026-01-18T11:11:32.294536Z",
     "iopub.status.idle": "2026-01-18T11:11:32.299029Z",
     "shell.execute_reply": "2026-01-18T11:11:32.298333Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_and_extract(temp_dir: str) -> list[Path]:\n",
    "    \"\"\"Download XLSX/ZIP files from USDA and extract to temp directory.\n",
    "\n",
    "    Note: BeautifulSoup's dynamic attribute access doesn't have complete type stubs,\n",
    "    so we use explicit str() conversion to satisfy type checkers.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching {LISTING_URL}\")\n",
    "    response = requests.get(LISTING_URL, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    download_links: list[str] = []\n",
    "    for link_tag in soup.find_all(\"a\", href=True):\n",
    "        href: str = str(link_tag[\"href\"])\n",
    "        # Strip query string before checking extension (handles \".xlsx?timestamp=123\" patterns)\n",
    "        base_href = href.lower().split(\"?\")[0]\n",
    "        if base_href.endswith((\".xlsx\", \".zip\")):\n",
    "            download_links.append(href)\n",
    "\n",
    "    print(f\"Found {len(download_links)} files to download\")\n",
    "    xlsx_files: list[Path] = []\n",
    "\n",
    "    for link in download_links:\n",
    "        url = link if link.startswith(\"http\") else f\"https://www.ers.usda.gov{link}\"\n",
    "        filename = url.split(\"/\")[-1].split(\"?\")[0]\n",
    "        local_path = Path(temp_dir) / filename\n",
    "\n",
    "        try:\n",
    "            print(f\"Downloading {filename}\")\n",
    "            file_response = requests.get(url, timeout=60)\n",
    "            file_response.raise_for_status()\n",
    "            local_path.write_bytes(file_response.content)\n",
    "\n",
    "            if filename.endswith(\".zip\"):\n",
    "                with ZipFile(local_path, \"r\") as zip_archive:\n",
    "                    for archived_name in zip_archive.namelist():\n",
    "                        if archived_name.endswith(\".xlsx\"):\n",
    "                            zip_archive.extract(archived_name, temp_dir)\n",
    "                            xlsx_files.append(Path(temp_dir) / archived_name)\n",
    "            else:\n",
    "                xlsx_files.append(local_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {filename}: {e}\")\n",
    "\n",
    "    return xlsx_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLSX Parsing\n",
    "\n",
    "Functions to parse XLSX files and extract price data from worksheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T11:11:32.301043Z",
     "iopub.status.busy": "2026-01-18T11:11:32.300908Z",
     "iopub.status.idle": "2026-01-18T11:11:32.305737Z",
     "shell.execute_reply": "2026-01-18T11:11:32.304903Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_header_row(rows: list[list[object]]) -> int:\n",
    "    \"\"\"Find header row index by looking for 'Form' and 'Average retail price'.\"\"\"\n",
    "    for i, row in enumerate(rows[:MAX_HEADER_SEARCH_ROWS]):\n",
    "        text = \" \".join(collapse_whitespace(str(v)) for v in row if v).lower()\n",
    "        if \"form\" in text and \"average retail price\" in text:\n",
    "            return i\n",
    "        # Check merged header (split across two rows)\n",
    "        if i + 1 < len(rows):\n",
    "            next_text = \" \".join(collapse_whitespace(str(v)) for v in rows[i + 1] if v).lower()\n",
    "            combined = text + \" \" + next_text\n",
    "            if \"form\" in combined and \"average retail price\" in combined:\n",
    "                return i + 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def extract_year(rows: list[list[object]], header_row_index: int, sheet_title: str, filename: str) -> int | None:\n",
    "    \"\"\"Extract year from title rows, sheet name, or filename (in priority order).\n",
    "\n",
    "    Uses walrus operator (:=) to search and capture in a single expression.\n",
    "    Sources are checked in order: title rows first (most reliable), then sheet name, then filename.\n",
    "    \"\"\"\n",
    "    sources = [str(row[0]) for row in rows[:header_row_index] if row and row[0]]\n",
    "    sources += [sheet_title, filename]\n",
    "    for source in sources:\n",
    "        if match := YEAR_REGEX.search(source):\n",
    "            return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_product_name(rows: list[list[object]], header_row_index: int, sheet_title: str) -> str:\n",
    "    \"\"\"Extract product name from title row or sheet name.\"\"\"\n",
    "    for row in rows[:header_row_index]:\n",
    "        if row and row[0]:\n",
    "            title = str(row[0]).strip()\n",
    "            # Split on em-dash or hyphen (USDA uses various dash styles)\n",
    "            for delim in (\"\\u2014\", \" - \", \" \\u2013 \"):\n",
    "                if delim in title:\n",
    "                    return title.split(delim)[0].strip()\n",
    "            if title:\n",
    "                return title\n",
    "    return sheet_title.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T11:11:32.308551Z",
     "iopub.status.busy": "2026-01-18T11:11:32.308344Z",
     "iopub.status.idle": "2026-01-18T11:11:32.314339Z",
     "shell.execute_reply": "2026-01-18T11:11:32.313122Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_data_row(\n",
    "    row: list[object], product_name: str, date_str: str, current_group: str | None\n",
    ") -> tuple[str, str] | None:\n",
    "    \"\"\"Parse a data row into (series_code, csv_line), (\"__GROUP__\", group_name), or None.\n",
    "\n",
    "    Returns:\n",
    "        - (series_code, csv_row): Valid data row\n",
    "        - (\"__GROUP__\", group_name): Group header (e.g., \"Fresh\", \"Canned\") - signals context change\n",
    "        - None: Skip row (footnote, source line, or insufficient data)\n",
    "    \"\"\"\n",
    "    if len(row) < MIN_DATA_ROW_COLUMNS:\n",
    "        return None\n",
    "\n",
    "    form_raw = str(row[0] or \"\").strip()\n",
    "    if not form_raw:\n",
    "        return None\n",
    "\n",
    "    # Skip non-data rows: footnotes (start with digit), source/contact lines\n",
    "    form_lower = form_raw.lower()\n",
    "    if form_raw[0].isdigit() or form_lower.startswith((\"source\", \"contact\", \"errata\")):\n",
    "        return None\n",
    "\n",
    "    # Parse numeric values first to determine if this is a data row or group header\n",
    "    avg_price = parse_decimal(row[1])\n",
    "    yield_factor = parse_decimal(row[3])\n",
    "    cup_size = parse_decimal(row[4])\n",
    "    price_per_cup = parse_decimal(row[6])\n",
    "    has_numeric_data = any(v is not None for v in (avg_price, yield_factor, cup_size, price_per_cup))\n",
    "\n",
    "    # Check for group headers (Fresh, Canned, etc.) - must have no numeric data\n",
    "    # Strip footnotes first (e.g., \"Fresh1\" → \"Fresh\") before checking\n",
    "    form_normalized = FOOTNOTE_REGEX.sub(\"\", form_raw).strip().lower()\n",
    "    if form_normalized in FORM_CATEGORY_LABELS and not has_numeric_data:\n",
    "        return (\"__GROUP__\", form_normalized.title())\n",
    "\n",
    "    # All numeric values missing = not a data row (but wasn't a group header either)\n",
    "    if not has_numeric_data:\n",
    "        return None\n",
    "\n",
    "    # Strip trailing footnote markers from form\n",
    "    form = FOOTNOTE_REGEX.sub(\"\", form_raw).strip()\n",
    "    if not form:\n",
    "        return None\n",
    "\n",
    "    # Apply group context (e.g., \"Florets\" -> \"Fresh, Florets\")\n",
    "    # But don't apply if form already contains the current group\n",
    "    if current_group:\n",
    "        form_lower_clean = form.lower()\n",
    "        if current_group.lower() not in form_lower_clean:\n",
    "            form = f\"{current_group}, {form}\"\n",
    "\n",
    "    # Parse units\n",
    "    price_unit = lookup_canonical_unit(row[2], PRICE_UNIT_MAP) if avg_price is not None else \"\"\n",
    "    cup_unit = lookup_canonical_unit(row[5], CUP_UNIT_MAP) if cup_size is not None else \"\"\n",
    "\n",
    "    # Normalize cup equivalent units (fluid_ounces → pints)\n",
    "    cup_size, cup_unit = normalize_cup_equivalent(cup_size, cup_unit)\n",
    "\n",
    "    series_code = make_series_code(product_name, form)\n",
    "    csv_row = f\"{date_str},{format_decimal_for_csv(avg_price)},{price_unit},{format_decimal_for_csv(yield_factor)},{format_decimal_for_csv(cup_size)},{cup_unit},{format_decimal_for_csv(price_per_cup)}\"\n",
    "\n",
    "    return series_code, csv_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T11:11:32.316108Z",
     "iopub.status.busy": "2026-01-18T11:11:32.315968Z",
     "iopub.status.idle": "2026-01-18T11:11:32.321505Z",
     "shell.execute_reply": "2026-01-18T11:11:32.320373Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_xlsx(file_path: Path, series_csv_rows: dict[str, dict[int, str]]) -> None:\n",
    "    \"\"\"Parse single XLSX file into series_csv_rows.\n",
    "\n",
    "    Note: openpyxl's cell.value has complex union types. We use list[list[object]]\n",
    "    as a practical type annotation that captures \"list of rows, each row is a list of cell values\".\n",
    "    \"\"\"\n",
    "    workbook = openpyxl.load_workbook(file_path, data_only=True)\n",
    "\n",
    "    for sheet in workbook.worksheets:\n",
    "        rows: list[list[object]] = [[cell.value for cell in row] for row in sheet.iter_rows()]\n",
    "        if not rows:\n",
    "            continue\n",
    "\n",
    "        header_row_index = find_header_row(rows)\n",
    "        if header_row_index < 0:\n",
    "            print(f\"Warning: No header found in {file_path.name} sheet {sheet.title}\")\n",
    "            continue\n",
    "\n",
    "        year = extract_year(rows, header_row_index, sheet.title, file_path.name)\n",
    "        if not year:\n",
    "            print(f\"Warning: No year found in {file_path.name} sheet {sheet.title}\")\n",
    "            continue\n",
    "\n",
    "        product_name = extract_product_name(rows, header_row_index, sheet.title)\n",
    "        # Offset by 1 year to prevent look-ahead bias (publication date unknown)\n",
    "        date_str = f\"{year + 1}0101\"\n",
    "        current_group = None  # Track group context (Fresh, Canned, etc.)\n",
    "\n",
    "        for row_idx, row in enumerate(rows[header_row_index + 1 :], start=header_row_index + 2):\n",
    "            try:\n",
    "                row_result = parse_data_row(row, product_name, date_str, current_group)\n",
    "                if row_result is None:\n",
    "                    continue\n",
    "                if row_result[0] == \"__GROUP__\":\n",
    "                    current_group = row_result[1]\n",
    "                    continue\n",
    "                series_code, csv_row = row_result\n",
    "                series_csv_rows[series_code][year] = csv_row\n",
    "            except ValueError as e:\n",
    "                print(f\"Warning: {file_path.name} sheet {sheet.title} row {row_idx}: {e}\")\n",
    "\n",
    "\n",
    "def parse_all_files(xlsx_files: list[Path]) -> dict[str, dict[int, str]]:\n",
    "    \"\"\"Parse all XLSX files and return data grouped by series code.\"\"\"\n",
    "    series_csv_rows: dict[str, dict[int, str]] = defaultdict(dict)\n",
    "    for file_path in xlsx_files:\n",
    "        try:\n",
    "            parse_xlsx(file_path, series_csv_rows)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_path.name}: {e}\")\n",
    "    return series_csv_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "Writes one CSV file per series, sorted by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T11:11:32.323339Z",
     "iopub.status.busy": "2026-01-18T11:11:32.323163Z",
     "iopub.status.idle": "2026-01-18T11:11:32.326435Z",
     "shell.execute_reply": "2026-01-18T11:11:32.325583Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_output(series_csv_rows: dict[str, dict[int, str]]) -> None:\n",
    "    \"\"\"Write CSV files for each series.\"\"\"\n",
    "    for series_code, rows_by_year in sorted(series_csv_rows.items()):\n",
    "        output_path = OUTPUT_DIR / f\"{series_code}.csv\"\n",
    "        csv_rows = [csv_row for _, csv_row in sorted(rows_by_year.items())]\n",
    "        output_path.write_text(\"\\n\".join(csv_rows))\n",
    "        print(f\"Wrote {len(csv_rows)} rows to {output_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Entry Point\n",
    "\n",
    "Run the full ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-18T11:11:32.328030Z",
     "iopub.status.busy": "2026-01-18T11:11:32.327899Z",
     "iopub.status.idle": "2026-01-18T11:13:21.205442Z",
     "shell.execute_reply": "2026-01-18T11:13:21.204583Z"
    }
   },
   "outputs": [],
   "source": [
    "def main() -> int:\n",
    "    print(\"USDA Fruit & Vegetables Data Processor\")\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        xlsx_files = download_and_extract(temp_dir)\n",
    "        if not xlsx_files:\n",
    "            print(\"Error: No XLSX files found\")\n",
    "            return 1\n",
    "\n",
    "        series_csv_rows = parse_all_files(xlsx_files)\n",
    "        if not series_csv_rows:\n",
    "            print(\"Error: No data parsed\")\n",
    "            return 1\n",
    "\n",
    "        write_output(series_csv_rows)\n",
    "\n",
    "    print(\"Processing complete\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Run the processor. Raise SystemExit only on failure so callers can detect errors.\n",
    "# Success (exit code 0) completes silently to avoid nbconvert treating it as an exception.\n",
    "_exit_code = main()\n",
    "if _exit_code != 0:\n",
    "    raise SystemExit(_exit_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
