{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USDA Fruit & Vegetables Data Processor\n",
    "\n",
    "This notebook implements the ETL pipeline for USDA Fruit and Vegetable retail price data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T07:49:28.053497Z",
     "iopub.status.busy": "2026-01-21T07:49:28.053358Z",
     "iopub.status.idle": "2026-01-21T07:49:28.233540Z",
     "shell.execute_reply": "2026-01-21T07:49:28.232745Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import openpyxl\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "These constants define the processing behavior:\n",
    "- `LISTING_URL`: USDA data products page with download links\n",
    "- `OUTPUT_DIR`: Destination for generated CSV files (layout: `alternative/usda/fruitandvegetables/{product}.csv`)\n",
    "- `YEAR_REGEX`: Matches 4-digit years (1900-2099) in titles/filenames\n",
    "- Unit mappings: Canonical forms for price and cup equivalent units\n",
    "\n",
    "**Output format**: One CSV file per product (e.g., `apples.csv`) containing all forms (Fresh, Applesauce, Juice, etc.) as rows.\n",
    "CSV columns: `Date,Form,AverageRetailPrice,Unit,PreparationYieldFactor,CupEquivalentSize,CupEquivalentUnit,PricePerCupEquivalent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T07:49:28.235463Z",
     "iopub.status.busy": "2026-01-21T07:49:28.235236Z",
     "iopub.status.idle": "2026-01-21T07:49:28.238977Z",
     "shell.execute_reply": "2026-01-21T07:49:28.238056Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Constants ---\n",
    "LISTING_URL = \"https://www.ers.usda.gov/data-products/fruit-and-vegetable-prices\"\n",
    "\n",
    "# Output directory: uses TEMP_OUTPUT_DIRECTORY env var if set (for automated/cloud runs),\n",
    "# otherwise defaults to ./output (interactive notebook use)\n",
    "OUTPUT_DIR = (\n",
    "    Path(os.environ.get(\"TEMP_OUTPUT_DIRECTORY\", \"/temp-output-directory\")) / \"alternative/usda/fruitandvegetables\"\n",
    ")\n",
    "\n",
    "YEAR_REGEX = re.compile(r\"\\b((?:19|20)\\d{2})\\b\")\n",
    "FOOTNOTE_REGEX = re.compile(r\"\\s*\\d+(?:,\\d+)*\\s*$\")\n",
    "FORM_CATEGORY_LABELS = {\n",
    "    \"fresh\",\n",
    "    \"canned\",\n",
    "    \"frozen\",\n",
    "    \"dried\",\n",
    "    \"juice\",\n",
    "    \"peas & carrots\",\n",
    "    \"green peas & carrots\",\n",
    "    \"succotash\",\n",
    "}\n",
    "\n",
    "# --- XLSX Structure Constants ---\n",
    "# Header rows appear at index 0-1; MAX_HEADER_SEARCH_ROWS provides safety margin.\n",
    "# Data rows have 7-9 columns; MIN_DATA_ROW_COLUMNS is the minimum for a valid row.\n",
    "MAX_HEADER_SEARCH_ROWS = 15\n",
    "MIN_DATA_ROW_COLUMNS = 7\n",
    "\n",
    "# Note: Extended descriptions like \"per pint (16 fluid ounces concentrate)\" match via substring.\n",
    "PRICE_UNIT_MAP = {\n",
    "    \"per pound\": \"per_pound\",\n",
    "    \"per pint\": \"per_pint\",\n",
    "}\n",
    "CUP_UNIT_MAP = {\n",
    "    \"pounds\": \"pounds\",\n",
    "    \"pound\": \"pounds\",\n",
    "    \"pints\": \"pints\",\n",
    "    \"fluid ounces\": \"fluid_ounces\",\n",
    "    \"fl oz\": \"fluid_ounces\",\n",
    "    \"fl. oz.\": \"fluid_ounces\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Utility functions for text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T07:49:28.240601Z",
     "iopub.status.busy": "2026-01-21T07:49:28.240482Z",
     "iopub.status.idle": "2026-01-21T07:49:28.244282Z",
     "shell.execute_reply": "2026-01-21T07:49:28.243648Z"
    }
   },
   "outputs": [],
   "source": [
    "def slugify(text: str) -> str:\n",
    "    \"\"\"Convert text to lowercase slug with underscores.\"\"\"\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"_\", text.lower()).strip(\"_\")\n",
    "\n",
    "\n",
    "def collapse_whitespace(text: str) -> str:\n",
    "    \"\"\"Collapse multiple whitespace characters into single spaces.\"\"\"\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "\n",
    "def normalize_form_separator(form: str) -> str:\n",
    "    \"\"\"Normalize form name separators to consistent ' - ' (space-hyphen-space).\n",
    "\n",
    "    This ensures consistent form names across years, regardless of whether the source\n",
    "    data uses commas, semicolons, or other separators. For example:\n",
    "    - \"Fresh; Boiled\" → \"Fresh - Boiled\"\n",
    "    - \"Fresh, Boiled\" → \"Fresh - Boiled\"\n",
    "    - \"Juice; Ready to drink\" → \"Juice - Ready to drink\"\n",
    "\n",
    "    Note: This runs AFTER comma sanitization (which converts ',' to ';' to avoid\n",
    "    breaking CSV parsing), so we only need to normalize '; ' patterns.\n",
    "    \"\"\"\n",
    "    # Normalize '; ' (semicolon-space) to ' - ' (space-hyphen-space)\n",
    "    # This catches both direct semicolons in source data and sanitized commas\n",
    "    return re.sub(r\";\\s*\", \" - \", form)\n",
    "\n",
    "\n",
    "def lookup_canonical_unit(raw_unit_text: object, unit_map: dict[str, str]) -> str:\n",
    "    \"\"\"Look up canonical unit form from raw XLSX text using substring matching.\"\"\"\n",
    "    if raw_unit_text is None:\n",
    "        return \"\"\n",
    "    text = collapse_whitespace(str(raw_unit_text)).lower()\n",
    "    return next((canonical for pattern, canonical in unit_map.items() if pattern in text), \"\")\n",
    "\n",
    "\n",
    "def cell_to_csv(value: object) -> str:\n",
    "    \"\"\"Convert cell value to CSV string. Empty/None becomes empty string.\"\"\"\n",
    "    if value is None:\n",
    "        return \"\"\n",
    "    text = str(value).strip()\n",
    "    return text if text else \"\"\n",
    "\n",
    "\n",
    "def normalize_cup_equivalent(size_str: str, unit: str) -> tuple[str, str]:\n",
    "    \"\"\"Normalize cup equivalent units: convert fluid_ounces to pints (16 fl oz = 1 pint).\"\"\"\n",
    "    if not size_str or unit != \"fluid_ounces\":\n",
    "        return size_str, unit\n",
    "    try:\n",
    "        size_pints = float(size_str) / 16.0\n",
    "        return str(size_pints).rstrip(\"0\").rstrip(\".\"), \"pints\"\n",
    "    except ValueError:\n",
    "        return size_str, unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download & Extract\n",
    "\n",
    "Downloads XLSX/ZIP files from the USDA website and extracts them to a temporary directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T07:49:28.245802Z",
     "iopub.status.busy": "2026-01-21T07:49:28.245691Z",
     "iopub.status.idle": "2026-01-21T07:49:28.249739Z",
     "shell.execute_reply": "2026-01-21T07:49:28.249006Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_and_extract(temp_dir: str) -> list[Path]:\n",
    "    \"\"\"Download XLSX/ZIP files from USDA and extract to temp directory.\n",
    "\n",
    "    Note: BeautifulSoup's dynamic attribute access doesn't have complete type stubs,\n",
    "    so we use explicit str() conversion to satisfy type checkers.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching {LISTING_URL}\")\n",
    "    response = requests.get(LISTING_URL, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    download_links: list[str] = []\n",
    "    for link_tag in soup.find_all(\"a\", href=True):\n",
    "        href: str = str(link_tag[\"href\"])\n",
    "        # Strip query string before checking extension (handles \".xlsx?timestamp=123\" patterns)\n",
    "        base_href = href.lower().split(\"?\")[0]\n",
    "        if base_href.endswith((\".xlsx\", \".zip\")):\n",
    "            download_links.append(href)\n",
    "\n",
    "    print(f\"Found {len(download_links)} files to download\")\n",
    "    xlsx_files: list[Path] = []\n",
    "\n",
    "    for link in download_links:\n",
    "        url = link if link.startswith(\"http\") else f\"https://www.ers.usda.gov{link}\"\n",
    "        filename = url.split(\"/\")[-1].split(\"?\")[0]\n",
    "        local_path = Path(temp_dir) / filename\n",
    "\n",
    "        try:\n",
    "            print(f\"Downloading {filename}\")\n",
    "            file_response = requests.get(url, timeout=60)\n",
    "            file_response.raise_for_status()\n",
    "            local_path.write_bytes(file_response.content)\n",
    "\n",
    "            if filename.endswith(\".zip\"):\n",
    "                with ZipFile(local_path, \"r\") as zip_archive:\n",
    "                    for archived_name in zip_archive.namelist():\n",
    "                        if archived_name.endswith(\".xlsx\"):\n",
    "                            zip_archive.extract(archived_name, temp_dir)\n",
    "                            xlsx_files.append(Path(temp_dir) / archived_name)\n",
    "            else:\n",
    "                xlsx_files.append(local_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {filename}: {e}\")\n",
    "\n",
    "    return xlsx_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLSX Parsing\n",
    "\n",
    "Functions to parse XLSX files and extract price data from worksheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T07:49:28.251005Z",
     "iopub.status.busy": "2026-01-21T07:49:28.250893Z",
     "iopub.status.idle": "2026-01-21T07:49:28.255036Z",
     "shell.execute_reply": "2026-01-21T07:49:28.254414Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_header_row(rows: list[list[object]]) -> int:\n",
    "    \"\"\"Find header row index by looking for 'Form' and 'Average retail price'.\"\"\"\n",
    "    for i, row in enumerate(rows[:MAX_HEADER_SEARCH_ROWS]):\n",
    "        text = \" \".join(collapse_whitespace(str(v)) for v in row if v).lower()\n",
    "        if \"form\" in text and \"average retail price\" in text:\n",
    "            return i\n",
    "        # Check merged header (split across two rows)\n",
    "        if i + 1 < len(rows):\n",
    "            next_text = \" \".join(collapse_whitespace(str(v)) for v in rows[i + 1] if v).lower()\n",
    "            combined = text + \" \" + next_text\n",
    "            if \"form\" in combined and \"average retail price\" in combined:\n",
    "                return i + 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def extract_year(rows: list[list[object]], header_row_index: int, sheet_title: str, filename: str) -> int | None:\n",
    "    \"\"\"Extract year from title rows, sheet name, or filename (in priority order).\n",
    "\n",
    "    Uses walrus operator (:=) to search and capture in a single expression.\n",
    "    Sources are checked in order: title rows first (most reliable), then sheet name, then filename.\n",
    "    \"\"\"\n",
    "    sources = [str(row[0]) for row in rows[:header_row_index] if row and row[0]]\n",
    "    sources += [sheet_title, filename]\n",
    "    for source in sources:\n",
    "        if match := YEAR_REGEX.search(source):\n",
    "            return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_product_name(rows: list[list[object]], header_row_index: int, sheet_title: str) -> str:\n",
    "    \"\"\"Extract product name from title row or sheet name.\"\"\"\n",
    "    for row in rows[:header_row_index]:\n",
    "        if row and row[0]:\n",
    "            title = str(row[0]).strip()\n",
    "            # Split on em-dash or hyphen (USDA uses various dash styles)\n",
    "            for delim in (\"\\u2014\", \" - \", \" \\u2013 \"):\n",
    "                if delim in title:\n",
    "                    return title.split(delim)[0].strip()\n",
    "            if title:\n",
    "                return title\n",
    "    return sheet_title.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T07:49:28.256273Z",
     "iopub.status.busy": "2026-01-21T07:49:28.256163Z",
     "iopub.status.idle": "2026-01-21T07:49:28.260466Z",
     "shell.execute_reply": "2026-01-21T07:49:28.259639Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_data_row(\n",
    "    row: list[object], product_name: str, date_str: str, current_group: str | None\n",
    ") -> tuple[str, str, str] | tuple[str, str] | None:\n",
    "    \"\"\"Parse a data row into (product_code, form, csv_line), (\"__GROUP__\", group_name), or None.\n",
    "\n",
    "    Returns:\n",
    "        - (product_code, form, csv_row): Valid data row with product code, form, and CSV data\n",
    "        - (\"__GROUP__\", group_name): Group header (e.g., \"Fresh\", \"Canned\") - signals context change\n",
    "        - None: Skip row (footnote, source line, or insufficient data)\n",
    "    \"\"\"\n",
    "    if len(row) < MIN_DATA_ROW_COLUMNS:\n",
    "        return None\n",
    "\n",
    "    form_raw = str(row[0] or \"\").strip()\n",
    "    if not form_raw:\n",
    "        return None\n",
    "\n",
    "    # Skip non-data rows: footnotes (start with digit), source/contact lines\n",
    "    form_lower = form_raw.lower()\n",
    "    if form_raw[0].isdigit() or form_lower.startswith((\"source\", \"contact\", \"errata\")):\n",
    "        return None\n",
    "\n",
    "    # Get raw string values for numeric columns\n",
    "    avg_price = cell_to_csv(row[1])\n",
    "    yield_factor = cell_to_csv(row[3])\n",
    "    cup_size = cell_to_csv(row[4])\n",
    "    price_per_cup = cell_to_csv(row[6])\n",
    "    has_numeric_data = any(v for v in (avg_price, yield_factor, cup_size, price_per_cup))\n",
    "\n",
    "    # Check for group headers (Fresh, Canned, etc.) - must have no numeric data\n",
    "    # Strip footnotes first (e.g., \"Fresh1\" → \"Fresh\") before checking\n",
    "    form_normalized = FOOTNOTE_REGEX.sub(\"\", form_raw).strip().lower()\n",
    "    if form_normalized in FORM_CATEGORY_LABELS and not has_numeric_data:\n",
    "        return (\"__GROUP__\", form_normalized.title())\n",
    "\n",
    "    # All numeric values missing = not a data row (but wasn't a group header either)\n",
    "    if not has_numeric_data:\n",
    "        return None\n",
    "\n",
    "    # Strip trailing footnote markers from form\n",
    "    form = FOOTNOTE_REGEX.sub(\"\", form_raw).strip()\n",
    "    if not form:\n",
    "        return None\n",
    "\n",
    "    # Apply group context (e.g., \"Florets\" -> \"Fresh - Florets\")\n",
    "    # Use hyphen instead of comma to avoid breaking CSV parsing\n",
    "    if current_group:\n",
    "        form_lower_clean = form.lower()\n",
    "        if current_group.lower() not in form_lower_clean:\n",
    "            form = f\"{current_group} - {form}\"\n",
    "\n",
    "    # Sanitize any remaining commas in form names (e.g., \"Juice, Ready to drink\")\n",
    "    # Replace with semicolon to preserve readability while avoiding CSV column issues\n",
    "    form = form.replace(\",\", \";\")\n",
    "\n",
    "    # Normalize form separators to consistent ' - ' pattern\n",
    "    # This ensures \"Fresh; Boiled\" (2024 format) matches \"Fresh - Boiled\" (earlier years)\n",
    "    form = normalize_form_separator(form)\n",
    "\n",
    "    # Parse units (only when corresponding value exists)\n",
    "    price_unit = lookup_canonical_unit(row[2], PRICE_UNIT_MAP) if avg_price else \"\"\n",
    "    cup_unit = lookup_canonical_unit(row[5], CUP_UNIT_MAP) if cup_size else \"\"\n",
    "\n",
    "    # Normalize cup equivalent units (fluid_ounces → pints)\n",
    "    cup_size, cup_unit = normalize_cup_equivalent(cup_size, cup_unit)\n",
    "\n",
    "    # Product code is just the product name slugified\n",
    "    product_code = slugify(product_name)\n",
    "\n",
    "    # CSV row now includes Form as the second column (after date which is prepended later)\n",
    "    # Format: Date,Form,AverageRetailPrice,Unit,PreparationYieldFactor,CupEquivalentSize,CupEquivalentUnit,PricePerCupEquivalent\n",
    "    csv_row = f\"{date_str},{form},{avg_price},{price_unit},{yield_factor},{cup_size},{cup_unit},{price_per_cup}\"\n",
    "\n",
    "    return product_code, form, csv_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T07:49:28.261742Z",
     "iopub.status.busy": "2026-01-21T07:49:28.261632Z",
     "iopub.status.idle": "2026-01-21T07:49:28.265836Z",
     "shell.execute_reply": "2026-01-21T07:49:28.265188Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_xlsx(file_path: Path, product_csv_rows: dict[str, list[str]]) -> None:\n",
    "    \"\"\"Parse single XLSX file into product_csv_rows.\n",
    "\n",
    "    Data is grouped by product code (not product+form). Each product file will contain\n",
    "    multiple forms as separate rows, with Form as the second column.\n",
    "\n",
    "    Note: openpyxl's cell.value has complex union types. We use list[list[object]]\n",
    "    as a practical type annotation that captures \"list of rows, each row is a list of cell values\".\n",
    "    \"\"\"\n",
    "    workbook = openpyxl.load_workbook(file_path, data_only=True)\n",
    "\n",
    "    for sheet in workbook.worksheets:\n",
    "        rows: list[list[object]] = [[cell.value for cell in row] for row in sheet.iter_rows()]\n",
    "        if not rows:\n",
    "            continue\n",
    "\n",
    "        header_row_index = find_header_row(rows)\n",
    "        if header_row_index < 0:\n",
    "            print(f\"Warning: No header found in {file_path.name} sheet {sheet.title}\")\n",
    "            continue\n",
    "\n",
    "        year = extract_year(rows, header_row_index, sheet.title, file_path.name)\n",
    "        if not year:\n",
    "            print(f\"Warning: No year found in {file_path.name} sheet {sheet.title}\")\n",
    "            continue\n",
    "\n",
    "        product_name = extract_product_name(rows, header_row_index, sheet.title)\n",
    "        # Offset by 1 year to prevent look-ahead bias (publication date unknown)\n",
    "        date_str = f\"{year + 1}0101\"\n",
    "        current_group = None  # Track group context (Fresh, Canned, etc.)\n",
    "\n",
    "        for row_idx, row in enumerate(rows[header_row_index + 1 :], start=header_row_index + 2):\n",
    "            try:\n",
    "                row_result = parse_data_row(row, product_name, date_str, current_group)\n",
    "                if row_result is None:\n",
    "                    continue\n",
    "                if row_result[0] == \"__GROUP__\":\n",
    "                    current_group = row_result[1]\n",
    "                    continue\n",
    "                # Unpack: (product_code, form, csv_row)\n",
    "                product_code, _form, csv_row = row_result\n",
    "                product_csv_rows[product_code].append(csv_row)\n",
    "            except ValueError as e:\n",
    "                print(f\"Warning: {file_path.name} sheet {sheet.title} row {row_idx}: {e}\")\n",
    "\n",
    "\n",
    "def parse_all_files(xlsx_files: list[Path]) -> dict[str, list[str]]:\n",
    "    \"\"\"Parse all XLSX files and return data grouped by product code.\n",
    "\n",
    "    Returns dict mapping product_code -> list of CSV rows (unsorted).\n",
    "    Each CSV row includes the date and form, so rows can be sorted later.\n",
    "    \"\"\"\n",
    "    product_csv_rows: dict[str, list[str]] = defaultdict(list)\n",
    "    for file_path in xlsx_files:\n",
    "        try:\n",
    "            parse_xlsx(file_path, product_csv_rows)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_path.name}: {e}\")\n",
    "    return product_csv_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "Writes one CSV file per product (not per form), containing all forms sorted by date then form name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T07:49:28.267074Z",
     "iopub.status.busy": "2026-01-21T07:49:28.266963Z",
     "iopub.status.idle": "2026-01-21T07:49:28.269705Z",
     "shell.execute_reply": "2026-01-21T07:49:28.269013Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_output(product_csv_rows: dict[str, list[str]]) -> None:\n",
    "    \"\"\"Write one CSV file per product, containing all forms sorted by date then form.\n",
    "\n",
    "    Each product file (e.g., apples.csv) contains rows for all forms (Fresh, Applesauce, etc.)\n",
    "    with CSV format: Date,Form,AverageRetailPrice,Unit,PreparationYieldFactor,CupEquivalentSize,CupEquivalentUnit,PricePerCupEquivalent\n",
    "    \"\"\"\n",
    "    for product_code, csv_rows in sorted(product_csv_rows.items()):\n",
    "        output_path = OUTPUT_DIR / f\"{product_code}.csv\"\n",
    "        # Sort by date (column 0), then by form (column 1) for consistent output\n",
    "        sorted_rows = sorted(csv_rows, key=lambda r: (r.split(\",\")[0], r.split(\",\")[1]))\n",
    "        output_path.write_text(\"\\n\".join(sorted_rows))\n",
    "        print(f\"Wrote {len(sorted_rows)} rows to {output_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Entry Point\n",
    "\n",
    "Run the full ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T07:49:28.270863Z",
     "iopub.status.busy": "2026-01-21T07:49:28.270766Z",
     "iopub.status.idle": "2026-01-21T07:51:17.491304Z",
     "shell.execute_reply": "2026-01-21T07:51:17.490573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USDA Fruit & Vegetables Data Processor\n",
      "Fetching https://www.ers.usda.gov/data-products/fruit-and-vegetable-prices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 82 files to download\n",
      "Downloading apples-average-retail-price-per-pound-or-pint-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading apricots-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading bananas-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading berries-mixed-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading blackberries-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading blueberries-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading cantaloupe-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading cherries-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading clementines-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading cranberries-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dates-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading figs-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading fruit-cocktail-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading grapefruit-average-retail-price-per-pound-or-pint-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading grapes-average-retail-price-per-pound-or-pint-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading honeydew-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading kiwi-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading mangoes-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading nectarines-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading oranges-average-retail-price-per-pound-or-pint-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading papaya-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading peaches-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pears-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pineapple-average-retail-price-per-pound-or-pint-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading plums-average-retail-price-per-pound-or-pint-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pomegranate-average-retail-price-per-pound-or-pint-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading raspberries-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading strawberries-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading watermelon-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading acorn-squash-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading artichoke-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading asparagus-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading avocados-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading beets-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading black-beans-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading blackeye-peas-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading broccoli-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading brussels-sprouts-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading butternut-squash-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading cabbage-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading carrots-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading cauliflower-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading celery-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading collard-greens-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading sweet-corn-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading cucumbers-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading great-northern-beans-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading green-beans-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading green-peas-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading green-peppers-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading kale-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading kidney-beans-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading lentils-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading iceberg-lettuce-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading romaine-lettuce-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading lima-beans-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading mixed-vegetables-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading mushrooms-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading mustard-greens-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading navy-beans-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading okra-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading olives-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading onions-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pinto-beans-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading potatoes-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pumpkin-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading radish-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading red-peppers-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading spinach-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading sweet-potatoes-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tomatoes-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading turnip-greens-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading zucchini-average-retail-price-per-pound-and-per-cup-equivalent.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading archived-2013-data-tables-for-fruit.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading archived-2016-data-tables-for-fruit.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading archived-2020-data-tables-for-fruit.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading archived-2022-data-tables-for-fruit.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading archived-2013-data-tables-for-vegetables.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading archived-2016-data-tables-for-vegetables.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading archived-2020-data-tables-for-vegetables.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading archived-2022-data-tables-for-vegetables.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading archived-data-tables-for-snack-substitutions.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 5 rows to acorn_squash.csv\n",
      "Wrote 20 rows to apples.csv\n",
      "Wrote 20 rows to apricots.csv\n",
      "Wrote 12 rows to artichoke.csv\n",
      "Wrote 15 rows to asparagus.csv\n",
      "Wrote 5 rows to avocados.csv\n",
      "Wrote 5 rows to bananas.csv\n",
      "Wrote 5 rows to beets.csv\n",
      "Wrote 10 rows to black_beans.csv\n",
      "Wrote 10 rows to blackberries.csv\n",
      "Wrote 10 rows to blackeye_peas.csv\n",
      "Wrote 10 rows to blueberries.csv\n",
      "Wrote 15 rows to broccoli.csv\n",
      "Wrote 10 rows to brussels_sprouts.csv\n",
      "Wrote 5 rows to butternut_squash.csv\n",
      "Wrote 15 rows to cabbage.csv\n",
      "Wrote 5 rows to cantaloupe.csv\n",
      "Wrote 25 rows to carrots.csv\n",
      "Wrote 15 rows to cauliflower.csv\n",
      "Wrote 10 rows to celery.csv\n",
      "Wrote 10 rows to cherries.csv\n",
      "Wrote 3 rows to clementines.csv\n",
      "Wrote 15 rows to collard_greens.csv\n",
      "Wrote 5 rows to cranberries.csv\n",
      "Wrote 10 rows to cucumbers.csv\n",
      "Wrote 5 rows to dates.csv\n",
      "Wrote 5 rows to figs.csv\n",
      "Wrote 10 rows to fruit_cocktail.csv\n",
      "Wrote 11 rows to grapefruit.csv\n",
      "Wrote 20 rows to grapes.csv\n",
      "Wrote 10 rows to great_northern_beans.csv\n",
      "Wrote 15 rows to green_beans.csv\n",
      "Wrote 10 rows to green_peas.csv\n",
      "Wrote 5 rows to green_peppers.csv\n",
      "Wrote 5 rows to honeydew_melon.csv\n",
      "Wrote 5 rows to iceberg_lettuce.csv\n",
      "Wrote 11 rows to kale.csv\n",
      "Wrote 10 rows to kidney_beans.csv\n",
      "Wrote 5 rows to kiwi.csv\n",
      "Wrote 5 rows to lentils.csv\n",
      "Wrote 15 rows to lima_beans.csv\n",
      "Wrote 10 rows to mangoes.csv\n",
      "Wrote 5 rows to mixed_berries.csv\n",
      "Wrote 20 rows to mixed_vegetables.csv\n",
      "Wrote 10 rows to mushrooms.csv\n",
      "Wrote 12 rows to mustard_greens.csv\n",
      "Wrote 10 rows to navy_beans.csv\n",
      "Wrote 5 rows to nectarines.csv\n",
      "Wrote 10 rows to okra.csv\n",
      "Wrote 5 rows to olives.csv\n",
      "Wrote 5 rows to onions.csv\n",
      "Wrote 15 rows to oranges.csv\n",
      "Wrote 10 rows to papaya.csv\n",
      "Wrote 20 rows to peaches.csv\n",
      "Wrote 15 rows to pears.csv\n",
      "Wrote 30 rows to pineapple.csv\n",
      "Wrote 10 rows to pinto_beans.csv\n",
      "Wrote 15 rows to plums.csv\n",
      "Wrote 10 rows to pomegranate.csv\n",
      "Wrote 15 rows to potatoes.csv\n",
      "Wrote 5 rows to pumpkin.csv\n",
      "Wrote 5 rows to radish.csv\n",
      "Wrote 10 rows to raspberries.csv\n",
      "Wrote 5 rows to red_peppers.csv\n",
      "Wrote 10 rows to romaine_lettuce.csv\n",
      "Wrote 20 rows to spinach.csv\n",
      "Wrote 10 rows to strawberries.csv\n",
      "Wrote 2 rows to summer_squash.csv\n",
      "Wrote 15 rows to sweet_corn.csv\n",
      "Wrote 5 rows to sweet_potatoes.csv\n",
      "Wrote 2 rows to tangerines.csv\n",
      "Wrote 20 rows to tomatoes.csv\n",
      "Wrote 15 rows to turnip_greens.csv\n",
      "Wrote 5 rows to watermelon.csv\n",
      "Wrote 3 rows to zucchini.csv\n",
      "Generated 75 product files\n",
      "Processing complete\n"
     ]
    }
   ],
   "source": [
    "def main() -> int:\n",
    "    print(\"USDA Fruit & Vegetables Data Processor\")\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        xlsx_files = download_and_extract(temp_dir)\n",
    "        if not xlsx_files:\n",
    "            print(\"Error: No XLSX files found\")\n",
    "            return 1\n",
    "\n",
    "        product_csv_rows = parse_all_files(xlsx_files)\n",
    "        if not product_csv_rows:\n",
    "            print(\"Error: No data parsed\")\n",
    "            return 1\n",
    "\n",
    "        write_output(product_csv_rows)\n",
    "        print(f\"Generated {len(product_csv_rows)} product files\")\n",
    "\n",
    "    print(\"Processing complete\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Run the processor. Raise SystemExit only on failure so callers can detect errors.\n",
    "# Success (exit code 0) completes silently to avoid nbconvert treating it as an exception.\n",
    "_exit_code = main()\n",
    "if _exit_code != 0:\n",
    "    raise SystemExit(_exit_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}